{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8a3ae4",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline\n",
    "\n",
    "from torch import nn, optim                           # torch 에서 제공하는 신경망 기술, 손실함수, 최적화를 할 수 있는 함수들을 불러온다.\n",
    "from torch.utils.data import DataLoader, Dataset      # 데이터를 모델에 사용할 수 있게 정리해주는 라이브러리.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Loss\n",
    "from sklearn.metrics import mean_squared_error        # regression 문제의 모델 성능 측정을 위해서 MSE를 불러온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b711680",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kp_inter', 'Ap_inter', 'F107_inter', 'SunSpot_inter', 'dst', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'C_D', 'S_D', 'C_H', 'S_H', 'tec_ex(T1)']\n",
      "['Kp_inter', 'Ap_inter', 'F107_inter', 'SunSpot_inter', 'dst', 'T1', 'T2', 'T3', 'T4', 'T5', 'T6', 'T7', 'T8', 'T9', 'T10', 'T11', 'T12', 'T13', 'T14', 'T15', 'T16', 'C_D', 'S_D', 'C_H', 'S_H', 'tec_ex(T1)']\n"
     ]
    }
   ],
   "source": [
    "# Read Data\n",
    "train = pd.read_csv(\"C:/Users/rihot/Desktop/Deep_learning/capston_assignment/Preprocessed_data/datafile_nan.csv\")\n",
    "\n",
    "# Index column drop\n",
    "# Index 열은 Quality에 영향을 주지 않음\n",
    "train = train.drop(['xq'], axis=1)\n",
    "\n",
    "# 데이터 타입에 따라 분류\n",
    "numerical_columns = train.select_dtypes(exclude='object').columns.tolist()\n",
    "\n",
    "# 확인을 위한 호출\n",
    "print(numerical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ec3e81b",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = train['tec_ex(T1)'].values\n",
    "X = []\n",
    "\n",
    "for i, rows in train.iterrows():\n",
    "    # 데이터 프레임을 가로 한줄씩 출력 row\n",
    "    X.append([ rows['Kp_inter'], rows['Ap_inter'], rows['F107_inter'], rows['SunSpot_inter'], rows['dst']\n",
    "                , rows['T1'], rows['T2'], rows['T3'], rows['T4'], rows['T5'], rows['T6'], rows['T7'], rows['T8'], rows['T9']\n",
    "                , rows['T10'], rows['T11'], rows['T12'], rows['T13'], rows['T14'], rows['T15'], rows['T16']\n",
    "                , rows['C_D'], rows['S_D'], rows['C_H'], rows['S_H']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84a5953b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = Y.reshape((-1,1)) # reshape(-1,1) 열 값은 지정했으니 알아서 행 지정해서 배열로 만들기\n",
    "\n",
    "# 데이터 스케일링\n",
    "# sklearn에서 제공하는 MinMaxScaler \n",
    "# (X-min(X))/(max(X)-min(X))을 계산\n",
    "scalerX = MinMaxScaler()\n",
    "scalerX.fit(X)\n",
    "X = scalerX.transform(X)\n",
    "\n",
    "scalerY = MinMaxScaler()\n",
    "scalerY.fit(Y)\n",
    "Y = scalerY.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f4c7e40f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# torch의 Dataset 을 상속.\n",
    "class TensorData(Dataset):\n",
    "\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.y_data = torch.FloatTensor(y_data)\n",
    "        self.len = self.y_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index] \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8ad165a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyper parameter tuning\n",
    "\n",
    "CFG = {\n",
    "    'EPOCHS':500, #에포크\n",
    "    'LEARNING_RATE':3e-4, #학습률\n",
    "    'BATCH_SIZE':16, #배치사이즈\n",
    "    'SEED':41, #시드\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2f9f3dd8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 전체 데이터를 학습 데이터와 평가 데이터로 나눈다.\n",
    "X_train, X_T, Y_train, Y_T = train_test_split(X, Y, test_size=0.4)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_T, Y_T, test_size=0.5)\n",
    "# train : val : test = 0.6 : 0.2 : 0.2\n",
    "\n",
    "# 학습 데이터, 시험 데이터 배치 형태로 구축하기\n",
    "trainsets = TensorData(X_train, Y_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainsets, batch_size=CFG['BATCH_SIZE'], shuffle=True)\n",
    "\n",
    "valsets = TensorData(X_val, Y_val)\n",
    "valloader = torch.utils.data.DataLoader(valsets, batch_size=CFG['BATCH_SIZE'], shuffle=False)\n",
    "\n",
    "testsets = TensorData(X_test, Y_test)\n",
    "testloader = torch.utils.data.DataLoader(testsets, batch_size=CFG['BATCH_SIZE'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6910834f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(25, 64, bias=False),\n",
    "            nn.BatchNorm1d(64, eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(64, 128, bias=False),\n",
    "            nn.BatchNorm1d(128, eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(128, 256, bias=False),\n",
    "            nn.BatchNorm1d(256, eps=1e-05, momentum=0.1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.layer4 = nn.Linear(256, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a57eae1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Regressor()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=CFG['LEARNING_RATE'], weight_decay=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7413de7b",
   "metadata": {
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_ = [] # loss 저장할 리스트\n",
    "val_loss_ = [] # val loss 저장할 리스트\n",
    "\n",
    "def train(model, optimizer, trainloader):\n",
    "    n = len(trainloader)\n",
    "    \n",
    "    # Loss Function\n",
    "    criterion = nn.MSELoss()\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(1, CFG[\"EPOCHS\"]+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, values = data\n",
    "            \n",
    "            optimizer.zero_grad() # 최적화 초기화\n",
    "            \n",
    "            outputs = model(inputs) # 예측값 산출\n",
    "            loss = criterion(outputs, values) # Error 계산\n",
    "            \n",
    "            loss.backward() # 역전파 진행\n",
    "            optimizer.step() # 역전파 진행 후 가중치 업데이트\n",
    "            \n",
    "            running_loss += loss.item() # Epoch 마다 평균 loss를 계산하기 위한 배치 loss\n",
    "                                        # item() 텐서로 값 받아오기\n",
    "        \n",
    "        loss_.append(running_loss/n) # MSE 계산\n",
    "        print('[%d] Train loss: %.10f' %(epoch, running_loss / len(trainloader)))\n",
    "        \n",
    "        #validation set 평가\n",
    "        model.eval() #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        predictions = torch.tensor([], dtype=torch.float) # 예측값을 저장하는 텐서.\n",
    "        actual = torch.tensor([], dtype=torch.float) # 실제값을 저장하는 텐서.\n",
    "        \n",
    "        with torch.no_grad(): #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
    "            for i, data in enumerate(valloader, 0):\n",
    "                inputs, values = data\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                val_loss += criterion(outputs, values)\n",
    "                \n",
    "                predictions = torch.cat((predictions, outputs), 0) # cat함수를 통해 예측값을 누적.\n",
    "                actual = torch.cat((actual, values), 0) # cat함수를 통해 실제값을 누적.\n",
    "\n",
    "                val_loss_.append(val_loss / n)\n",
    "\n",
    "                if i == len(valloader):\n",
    "                    torch.save(model.state_dict(), \"C:/Users/rihot/Desktop/Deep_learning/capston_assignment/last_model.pth\")\n",
    "        \n",
    "        predictions = predictions.numpy() # 넘파이 배열로 변경.\n",
    "        actual = actual.numpy() # 넘파이 배열로 변경.\n",
    "        rmse = np.sqrt(mean_squared_error(predictions, actual)) # sklearn을 이용해 RMSE를 계산.\n",
    "        print(f'val rmse:{rmse}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1cf6390",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Train loss: 0.0280752903\n",
      "test rmse:0.0978846624493599\n",
      "[2] Train loss: 0.0143849628\n",
      "test rmse:0.0825275406241417\n",
      "[3] Train loss: 0.0110570675\n",
      "test rmse:0.07629279047250748\n",
      "[4] Train loss: 0.0094572878\n",
      "test rmse:0.06935951113700867\n",
      "[5] Train loss: 0.0085661333\n",
      "test rmse:0.07322986423969269\n",
      "[6] Train loss: 0.0081437482\n",
      "test rmse:0.07057426124811172\n",
      "[7] Train loss: 0.0072937420\n",
      "test rmse:0.0683363825082779\n",
      "[8] Train loss: 0.0069935788\n",
      "test rmse:0.060495320707559586\n",
      "[9] Train loss: 0.0068117572\n",
      "test rmse:0.060628145933151245\n",
      "[10] Train loss: 0.0063076014\n",
      "test rmse:0.05600212514400482\n",
      "[11] Train loss: 0.0064882651\n",
      "test rmse:0.058224789798259735\n",
      "[12] Train loss: 0.0059762407\n",
      "test rmse:0.06257019191980362\n",
      "[13] Train loss: 0.0053733510\n",
      "test rmse:0.05324079468846321\n",
      "[14] Train loss: 0.0050452305\n",
      "test rmse:0.05360894650220871\n",
      "[15] Train loss: 0.0051672611\n",
      "test rmse:0.05233246088027954\n",
      "[16] Train loss: 0.0049786655\n",
      "test rmse:0.06268136948347092\n",
      "[17] Train loss: 0.0045699479\n",
      "test rmse:0.06105794757604599\n",
      "[18] Train loss: 0.0053988924\n",
      "test rmse:0.06357580423355103\n",
      "[19] Train loss: 0.0048998897\n",
      "test rmse:0.05917466804385185\n",
      "[20] Train loss: 0.0048225536\n",
      "test rmse:0.07086168229579926\n",
      "[21] Train loss: 0.0047046447\n",
      "test rmse:0.060362834483385086\n",
      "[22] Train loss: 0.0047720015\n",
      "test rmse:0.05526464059948921\n",
      "[23] Train loss: 0.0042531760\n",
      "test rmse:0.05078548192977905\n",
      "[24] Train loss: 0.0045206328\n",
      "test rmse:0.05242375284433365\n",
      "[25] Train loss: 0.0044767812\n",
      "test rmse:0.047241490334272385\n",
      "[26] Train loss: 0.0039457969\n",
      "test rmse:0.04750373587012291\n",
      "[27] Train loss: 0.0039822104\n",
      "test rmse:0.05720170959830284\n",
      "[28] Train loss: 0.0040916358\n",
      "test rmse:0.053166713565588\n",
      "[29] Train loss: 0.0041408136\n",
      "test rmse:0.052531056106090546\n",
      "[30] Train loss: 0.0037964671\n",
      "test rmse:0.05230090767145157\n",
      "[31] Train loss: 0.0038416689\n",
      "test rmse:0.04613050818443298\n",
      "[32] Train loss: 0.0034881698\n",
      "test rmse:0.05067847669124603\n",
      "[33] Train loss: 0.0040480382\n",
      "test rmse:0.0723685473203659\n",
      "[34] Train loss: 0.0036003526\n",
      "test rmse:0.04880162701010704\n",
      "[35] Train loss: 0.0032361712\n",
      "test rmse:0.05055040493607521\n",
      "[36] Train loss: 0.0036229101\n",
      "test rmse:0.05821853131055832\n",
      "[37] Train loss: 0.0035040159\n",
      "test rmse:0.04794134572148323\n",
      "[38] Train loss: 0.0031320065\n",
      "test rmse:0.04558074101805687\n",
      "[39] Train loss: 0.0032157529\n",
      "test rmse:0.05000542476773262\n",
      "[40] Train loss: 0.0036203619\n",
      "test rmse:0.050080541521310806\n",
      "[41] Train loss: 0.0035115591\n",
      "test rmse:0.05830687656998634\n",
      "[42] Train loss: 0.0032003570\n",
      "test rmse:0.044641170650720596\n",
      "[43] Train loss: 0.0031534833\n",
      "test rmse:0.04951639100909233\n",
      "[44] Train loss: 0.0037390424\n",
      "test rmse:0.04841739684343338\n",
      "[45] Train loss: 0.0030538551\n",
      "test rmse:0.041273701936006546\n",
      "[46] Train loss: 0.0028747627\n",
      "test rmse:0.0491865836083889\n",
      "[47] Train loss: 0.0031454831\n",
      "test rmse:0.046185869723558426\n",
      "[48] Train loss: 0.0027367006\n",
      "test rmse:0.04457437992095947\n",
      "[49] Train loss: 0.0031283540\n",
      "test rmse:0.04335400462150574\n",
      "[50] Train loss: 0.0027581601\n",
      "test rmse:0.045415349304676056\n",
      "[51] Train loss: 0.0027484171\n",
      "test rmse:0.04426239803433418\n",
      "[52] Train loss: 0.0028171883\n",
      "test rmse:0.04345722123980522\n",
      "[53] Train loss: 0.0027788607\n",
      "test rmse:0.04291331395506859\n",
      "[54] Train loss: 0.0025174736\n",
      "test rmse:0.040795911103487015\n",
      "[55] Train loss: 0.0027898407\n",
      "test rmse:0.03851080313324928\n",
      "[56] Train loss: 0.0027484073\n",
      "test rmse:0.049014512449502945\n",
      "[57] Train loss: 0.0029376130\n",
      "test rmse:0.05266103148460388\n",
      "[58] Train loss: 0.0029638379\n",
      "test rmse:0.05072450265288353\n",
      "[59] Train loss: 0.0029422881\n",
      "test rmse:0.04196157678961754\n",
      "[60] Train loss: 0.0024145358\n",
      "test rmse:0.05290906876325607\n",
      "[61] Train loss: 0.0028215414\n",
      "test rmse:0.06786332279443741\n",
      "[62] Train loss: 0.0027031742\n",
      "test rmse:0.04569230228662491\n",
      "[63] Train loss: 0.0025329501\n",
      "test rmse:0.044154830276966095\n",
      "[64] Train loss: 0.0027043536\n",
      "test rmse:0.043945010751485825\n",
      "[65] Train loss: 0.0026276069\n",
      "test rmse:0.05194418877363205\n",
      "[66] Train loss: 0.0024930854\n",
      "test rmse:0.0628010481595993\n",
      "[67] Train loss: 0.0026963070\n",
      "test rmse:0.04121857509016991\n",
      "[68] Train loss: 0.0025117726\n",
      "test rmse:0.042402368038892746\n",
      "[69] Train loss: 0.0026262368\n",
      "test rmse:0.05630291625857353\n",
      "[70] Train loss: 0.0026936721\n",
      "test rmse:0.04865531250834465\n",
      "[71] Train loss: 0.0025787897\n",
      "test rmse:0.054250042885541916\n",
      "[72] Train loss: 0.0023773972\n",
      "test rmse:0.05064430087804794\n",
      "[73] Train loss: 0.0024158352\n",
      "test rmse:0.045151881873607635\n",
      "[74] Train loss: 0.0023487690\n",
      "test rmse:0.03731180354952812\n",
      "[75] Train loss: 0.0025357133\n",
      "test rmse:0.03927459940314293\n",
      "[76] Train loss: 0.0024139322\n",
      "test rmse:0.04487749561667442\n",
      "[77] Train loss: 0.0022764011\n",
      "test rmse:0.04808312654495239\n",
      "[78] Train loss: 0.0022858600\n",
      "test rmse:0.04650377109646797\n",
      "[79] Train loss: 0.0019592933\n",
      "test rmse:0.03945983946323395\n",
      "[80] Train loss: 0.0022703545\n",
      "test rmse:0.0393218919634819\n",
      "[81] Train loss: 0.0023021416\n",
      "test rmse:0.047748204320669174\n",
      "[82] Train loss: 0.0020817601\n",
      "test rmse:0.03673835098743439\n",
      "[83] Train loss: 0.0020956558\n",
      "test rmse:0.03746157884597778\n",
      "[84] Train loss: 0.0020320397\n",
      "test rmse:0.03978974372148514\n",
      "[85] Train loss: 0.0022054233\n",
      "test rmse:0.04765763133764267\n",
      "[86] Train loss: 0.0024281767\n",
      "test rmse:0.04675745218992233\n",
      "[87] Train loss: 0.0019586064\n",
      "test rmse:0.03889063373208046\n",
      "[88] Train loss: 0.0020300940\n",
      "test rmse:0.04123585671186447\n",
      "[89] Train loss: 0.0020289585\n",
      "test rmse:0.03951839730143547\n",
      "[90] Train loss: 0.0019407122\n",
      "test rmse:0.03981129080057144\n",
      "[91] Train loss: 0.0021800258\n",
      "test rmse:0.037126269191503525\n",
      "[92] Train loss: 0.0022449910\n",
      "test rmse:0.03821409493684769\n",
      "[93] Train loss: 0.0019094365\n",
      "test rmse:0.039123427122831345\n",
      "[94] Train loss: 0.0020744549\n",
      "test rmse:0.04334544017910957\n",
      "[95] Train loss: 0.0020462880\n",
      "test rmse:0.040222544223070145\n",
      "[96] Train loss: 0.0017513658\n",
      "test rmse:0.03766976296901703\n",
      "[97] Train loss: 0.0018602517\n",
      "test rmse:0.040587835013866425\n",
      "[98] Train loss: 0.0015953013\n",
      "test rmse:0.03610875457525253\n",
      "[99] Train loss: 0.0017905794\n",
      "test rmse:0.0363641083240509\n",
      "[100] Train loss: 0.0020216563\n",
      "test rmse:0.03583960607647896\n",
      "[101] Train loss: 0.0019719791\n",
      "test rmse:0.04661877080798149\n",
      "[102] Train loss: 0.0020559625\n",
      "test rmse:0.037222396582365036\n",
      "[103] Train loss: 0.0016377146\n",
      "test rmse:0.03577207401394844\n",
      "[104] Train loss: 0.0019272325\n",
      "test rmse:0.04235139489173889\n",
      "[105] Train loss: 0.0015308659\n",
      "test rmse:0.03494485095143318\n",
      "[106] Train loss: 0.0016661350\n",
      "test rmse:0.041672058403491974\n",
      "[107] Train loss: 0.0017257677\n",
      "test rmse:0.04390112683176994\n",
      "[108] Train loss: 0.0018229714\n",
      "test rmse:0.03709620609879494\n",
      "[109] Train loss: 0.0017649222\n",
      "test rmse:0.03368254750967026\n",
      "[110] Train loss: 0.0016229630\n",
      "test rmse:0.03489089757204056\n",
      "[111] Train loss: 0.0016497178\n",
      "test rmse:0.03939727693796158\n",
      "[112] Train loss: 0.0017087196\n",
      "test rmse:0.038148943334817886\n",
      "[113] Train loss: 0.0015732190\n",
      "test rmse:0.03263947367668152\n",
      "[114] Train loss: 0.0015860404\n",
      "test rmse:0.03863801062107086\n",
      "[115] Train loss: 0.0019440051\n",
      "test rmse:0.036747340112924576\n",
      "[116] Train loss: 0.0015351416\n",
      "test rmse:0.036185380071401596\n",
      "[117] Train loss: 0.0016271421\n",
      "test rmse:0.040458619594573975\n",
      "[118] Train loss: 0.0015321370\n",
      "test rmse:0.03829237446188927\n",
      "[119] Train loss: 0.0016539015\n",
      "test rmse:0.04138938710093498\n",
      "[120] Train loss: 0.0015667251\n",
      "test rmse:0.03338088467717171\n",
      "[121] Train loss: 0.0015254351\n",
      "test rmse:0.03556213527917862\n",
      "[122] Train loss: 0.0015689150\n",
      "test rmse:0.04447432607412338\n",
      "[123] Train loss: 0.0014227523\n",
      "test rmse:0.037423934787511826\n",
      "[124] Train loss: 0.0013098644\n",
      "test rmse:0.030636200681328773\n",
      "[125] Train loss: 0.0015248177\n",
      "test rmse:0.034059710800647736\n",
      "[126] Train loss: 0.0014392017\n",
      "test rmse:0.038465917110443115\n",
      "[127] Train loss: 0.0013907274\n",
      "test rmse:0.039526429027318954\n",
      "[128] Train loss: 0.0014983809\n",
      "test rmse:0.04559925198554993\n",
      "[129] Train loss: 0.0013404452\n",
      "test rmse:0.04050825163722038\n",
      "[130] Train loss: 0.0015509080\n",
      "test rmse:0.035383403301239014\n",
      "[131] Train loss: 0.0014907502\n",
      "test rmse:0.03933974355459213\n",
      "[132] Train loss: 0.0012086737\n",
      "test rmse:0.03583001345396042\n",
      "[133] Train loss: 0.0015190334\n",
      "test rmse:0.034621428698301315\n",
      "[134] Train loss: 0.0013337810\n",
      "test rmse:0.034935712814331055\n",
      "[135] Train loss: 0.0015689184\n",
      "test rmse:0.03336174041032791\n",
      "[136] Train loss: 0.0018569448\n",
      "test rmse:0.045737896114587784\n",
      "[137] Train loss: 0.0016657480\n",
      "test rmse:0.0307221207767725\n",
      "[138] Train loss: 0.0015816210\n",
      "test rmse:0.033689115196466446\n",
      "[139] Train loss: 0.0014869127\n",
      "test rmse:0.038155246526002884\n",
      "[140] Train loss: 0.0012090971\n",
      "test rmse:0.03139377012848854\n",
      "[141] Train loss: 0.0014689717\n",
      "test rmse:0.027866315096616745\n",
      "[142] Train loss: 0.0014227531\n",
      "test rmse:0.04092860221862793\n",
      "[143] Train loss: 0.0017732173\n",
      "test rmse:0.03185780346393585\n",
      "[144] Train loss: 0.0012230170\n",
      "test rmse:0.032496511936187744\n",
      "[145] Train loss: 0.0012807960\n",
      "test rmse:0.03274325281381607\n",
      "[146] Train loss: 0.0011328964\n",
      "test rmse:0.03296665474772453\n",
      "[147] Train loss: 0.0014166248\n",
      "test rmse:0.03398387134075165\n",
      "[148] Train loss: 0.0013850875\n",
      "test rmse:0.029342371970415115\n",
      "[149] Train loss: 0.0015312001\n",
      "test rmse:0.030979495495557785\n",
      "[150] Train loss: 0.0015005464\n",
      "test rmse:0.03127160668373108\n",
      "[151] Train loss: 0.0012510807\n",
      "test rmse:0.03275810554623604\n",
      "[152] Train loss: 0.0013528159\n",
      "test rmse:0.031828928738832474\n",
      "[153] Train loss: 0.0019960838\n",
      "test rmse:0.03750205039978027\n",
      "[154] Train loss: 0.0019203977\n",
      "test rmse:0.03197987377643585\n",
      "[155] Train loss: 0.0013527169\n",
      "test rmse:0.0267584640532732\n",
      "[156] Train loss: 0.0011671466\n",
      "test rmse:0.0370914489030838\n",
      "[157] Train loss: 0.0011688024\n",
      "test rmse:0.03817957639694214\n",
      "[158] Train loss: 0.0013736213\n",
      "test rmse:0.04105658084154129\n",
      "[159] Train loss: 0.0012997756\n",
      "test rmse:0.03446485474705696\n",
      "[160] Train loss: 0.0014377279\n",
      "test rmse:0.029015572741627693\n",
      "[161] Train loss: 0.0013135077\n",
      "test rmse:0.035248637199401855\n",
      "[162] Train loss: 0.0012123364\n",
      "test rmse:0.03146830201148987\n",
      "[163] Train loss: 0.0021668786\n",
      "test rmse:0.036868467926979065\n",
      "[164] Train loss: 0.0013184326\n",
      "test rmse:0.029440511018037796\n",
      "[165] Train loss: 0.0012215795\n",
      "test rmse:0.032372139394283295\n",
      "[166] Train loss: 0.0011764262\n",
      "test rmse:0.02865862287580967\n",
      "[167] Train loss: 0.0010905379\n",
      "test rmse:0.031606778502464294\n",
      "[168] Train loss: 0.0011246825\n",
      "test rmse:0.032418154180049896\n",
      "[169] Train loss: 0.0011499858\n",
      "test rmse:0.03024098090827465\n",
      "[170] Train loss: 0.0013935052\n",
      "test rmse:0.027843328192830086\n",
      "[171] Train loss: 0.0017399969\n",
      "test rmse:0.030095400288701057\n",
      "[172] Train loss: 0.0013694904\n",
      "test rmse:0.031663767993450165\n",
      "[173] Train loss: 0.0013609119\n",
      "test rmse:0.03184227645397186\n",
      "[174] Train loss: 0.0012641377\n",
      "test rmse:0.03091331571340561\n",
      "[175] Train loss: 0.0010889339\n",
      "test rmse:0.030141392722725868\n",
      "[176] Train loss: 0.0011257379\n",
      "test rmse:0.032216791063547134\n",
      "[177] Train loss: 0.0009209987\n",
      "test rmse:0.02907443977892399\n",
      "[178] Train loss: 0.0010526398\n",
      "test rmse:0.03023514710366726\n",
      "[179] Train loss: 0.0010042217\n",
      "test rmse:0.031056001782417297\n",
      "[180] Train loss: 0.0012964800\n",
      "test rmse:0.031042689457535744\n",
      "[181] Train loss: 0.0010850076\n",
      "test rmse:0.028514767065644264\n",
      "[182] Train loss: 0.0012763468\n",
      "test rmse:0.030890459194779396\n",
      "[183] Train loss: 0.0011544078\n",
      "test rmse:0.028589194640517235\n",
      "[184] Train loss: 0.0012529245\n",
      "test rmse:0.028176654130220413\n",
      "[185] Train loss: 0.0013598186\n",
      "test rmse:0.04716107249259949\n",
      "[186] Train loss: 0.0011637440\n",
      "test rmse:0.029300682246685028\n",
      "[187] Train loss: 0.0010826189\n",
      "test rmse:0.028365451842546463\n",
      "[188] Train loss: 0.0012207017\n",
      "test rmse:0.03777686133980751\n",
      "[189] Train loss: 0.0011786058\n",
      "test rmse:0.0346529558300972\n",
      "[190] Train loss: 0.0011085923\n",
      "test rmse:0.03909870982170105\n",
      "[191] Train loss: 0.0015209333\n",
      "test rmse:0.030218277126550674\n",
      "[192] Train loss: 0.0017330969\n",
      "test rmse:0.031092189252376556\n",
      "[193] Train loss: 0.0010325713\n",
      "test rmse:0.02721145749092102\n",
      "[194] Train loss: 0.0012089487\n",
      "test rmse:0.02882634662091732\n",
      "[195] Train loss: 0.0010623665\n",
      "test rmse:0.030306972563266754\n",
      "[196] Train loss: 0.0010663778\n",
      "test rmse:0.0323479138314724\n",
      "[197] Train loss: 0.0010748078\n",
      "test rmse:0.031188789755105972\n",
      "[198] Train loss: 0.0012037136\n",
      "test rmse:0.031097309663891792\n",
      "[199] Train loss: 0.0009988549\n",
      "test rmse:0.026678960770368576\n",
      "[200] Train loss: 0.0011688261\n",
      "test rmse:0.035004548728466034\n",
      "[201] Train loss: 0.0010300297\n",
      "test rmse:0.02655087783932686\n",
      "[202] Train loss: 0.0011562407\n",
      "test rmse:0.03459254652261734\n",
      "[203] Train loss: 0.0010533875\n",
      "test rmse:0.03213324397802353\n",
      "[204] Train loss: 0.0011280462\n",
      "test rmse:0.027381347492337227\n",
      "[205] Train loss: 0.0010336308\n",
      "test rmse:0.042386721819639206\n",
      "[206] Train loss: 0.0010580411\n",
      "test rmse:0.028299203142523766\n",
      "[207] Train loss: 0.0009330043\n",
      "test rmse:0.03011593036353588\n",
      "[208] Train loss: 0.0009326028\n",
      "test rmse:0.02358655072748661\n",
      "[209] Train loss: 0.0009696105\n",
      "test rmse:0.029842762276530266\n",
      "[210] Train loss: 0.0012017639\n",
      "test rmse:0.02476550079882145\n",
      "[211] Train loss: 0.0012760039\n",
      "test rmse:0.0366620309650898\n",
      "[212] Train loss: 0.0010001164\n",
      "test rmse:0.03019842319190502\n",
      "[213] Train loss: 0.0011088322\n",
      "test rmse:0.030164917930960655\n",
      "[214] Train loss: 0.0012642742\n",
      "test rmse:0.029098408296704292\n",
      "[215] Train loss: 0.0009264218\n",
      "test rmse:0.03234390914440155\n",
      "[216] Train loss: 0.0009531136\n",
      "test rmse:0.03449459746479988\n",
      "[217] Train loss: 0.0009241149\n",
      "test rmse:0.027036506682634354\n",
      "[218] Train loss: 0.0010159561\n",
      "test rmse:0.02876194939017296\n",
      "[219] Train loss: 0.0010972440\n",
      "test rmse:0.032371047884225845\n",
      "[220] Train loss: 0.0009593776\n",
      "test rmse:0.032202281057834625\n",
      "[221] Train loss: 0.0008588651\n",
      "test rmse:0.027242077514529228\n",
      "[222] Train loss: 0.0009101743\n",
      "test rmse:0.029850171878933907\n",
      "[223] Train loss: 0.0009564805\n",
      "test rmse:0.028401236981153488\n",
      "[224] Train loss: 0.0011334426\n",
      "test rmse:0.03016642853617668\n",
      "[225] Train loss: 0.0012701649\n",
      "test rmse:0.023718474432826042\n",
      "[226] Train loss: 0.0009699248\n",
      "test rmse:0.03194243460893631\n",
      "[227] Train loss: 0.0009785309\n",
      "test rmse:0.033564891666173935\n",
      "[228] Train loss: 0.0008568799\n",
      "test rmse:0.026734676212072372\n",
      "[229] Train loss: 0.0009274891\n",
      "test rmse:0.029233543202280998\n",
      "[230] Train loss: 0.0008588604\n",
      "test rmse:0.023340357467532158\n",
      "[231] Train loss: 0.0010424103\n",
      "test rmse:0.026909122243523598\n",
      "[232] Train loss: 0.0011466522\n",
      "test rmse:0.0333041176199913\n",
      "[233] Train loss: 0.0012247758\n",
      "test rmse:0.03272019699215889\n",
      "[234] Train loss: 0.0009420140\n",
      "test rmse:0.02383453957736492\n",
      "[235] Train loss: 0.0008336069\n",
      "test rmse:0.02500302717089653\n",
      "[236] Train loss: 0.0011267143\n",
      "test rmse:0.02942824736237526\n",
      "[237] Train loss: 0.0008143661\n",
      "test rmse:0.02771841362118721\n",
      "[238] Train loss: 0.0013481010\n",
      "test rmse:0.03098701871931553\n",
      "[239] Train loss: 0.0009367206\n",
      "test rmse:0.029534604400396347\n",
      "[240] Train loss: 0.0010234533\n",
      "test rmse:0.027607696130871773\n",
      "[241] Train loss: 0.0008576725\n",
      "test rmse:0.024092894047498703\n",
      "[242] Train loss: 0.0008958246\n",
      "test rmse:0.028728976845741272\n",
      "[243] Train loss: 0.0008359974\n",
      "test rmse:0.026559188961982727\n",
      "[244] Train loss: 0.0010205574\n",
      "test rmse:0.02503373846411705\n",
      "[245] Train loss: 0.0008606457\n",
      "test rmse:0.027152210474014282\n",
      "[246] Train loss: 0.0014315954\n",
      "test rmse:0.04055533930659294\n",
      "[247] Train loss: 0.0011546741\n",
      "test rmse:0.03476564586162567\n",
      "[248] Train loss: 0.0007635734\n",
      "test rmse:0.024626895785331726\n",
      "[249] Train loss: 0.0009270051\n",
      "test rmse:0.026847081258893013\n",
      "[250] Train loss: 0.0011646848\n",
      "test rmse:0.02650114707648754\n",
      "[251] Train loss: 0.0009162298\n",
      "test rmse:0.024695832282304764\n",
      "[252] Train loss: 0.0010388474\n",
      "test rmse:0.026815345510840416\n",
      "[253] Train loss: 0.0009985320\n",
      "test rmse:0.027019621804356575\n",
      "[254] Train loss: 0.0008591042\n",
      "test rmse:0.030301738530397415\n",
      "[255] Train loss: 0.0008288982\n",
      "test rmse:0.025617100298404694\n",
      "[256] Train loss: 0.0009639418\n",
      "test rmse:0.026262402534484863\n",
      "[257] Train loss: 0.0009049732\n",
      "test rmse:0.02726045995950699\n",
      "[258] Train loss: 0.0008796218\n",
      "test rmse:0.02531460113823414\n",
      "[259] Train loss: 0.0008902980\n",
      "test rmse:0.03913707286119461\n",
      "[260] Train loss: 0.0008711823\n",
      "test rmse:0.028580352663993835\n",
      "[261] Train loss: 0.0008474828\n",
      "test rmse:0.02299138717353344\n",
      "[262] Train loss: 0.0008073562\n",
      "test rmse:0.023496124893426895\n",
      "[263] Train loss: 0.0007220034\n",
      "test rmse:0.02958374284207821\n",
      "[264] Train loss: 0.0009556835\n",
      "test rmse:0.023047026246786118\n",
      "[265] Train loss: 0.0013063454\n",
      "test rmse:0.02436606027185917\n",
      "[266] Train loss: 0.0009301970\n",
      "test rmse:0.02503904141485691\n",
      "[267] Train loss: 0.0009099851\n",
      "test rmse:0.028216268867254257\n",
      "[268] Train loss: 0.0007810597\n",
      "test rmse:0.02469548210501671\n",
      "[269] Train loss: 0.0008093897\n",
      "test rmse:0.03665407747030258\n",
      "[270] Train loss: 0.0008896714\n",
      "test rmse:0.02991129644215107\n",
      "[271] Train loss: 0.0009594204\n",
      "test rmse:0.02717539668083191\n",
      "[272] Train loss: 0.0008601964\n",
      "test rmse:0.02445519156754017\n",
      "[273] Train loss: 0.0008458829\n",
      "test rmse:0.03453851118683815\n",
      "[274] Train loss: 0.0008670065\n",
      "test rmse:0.024548886343836784\n",
      "[275] Train loss: 0.0008010565\n",
      "test rmse:0.025488080456852913\n",
      "[276] Train loss: 0.0006652784\n",
      "test rmse:0.026896364986896515\n",
      "[277] Train loss: 0.0007996743\n",
      "test rmse:0.024327775463461876\n",
      "[278] Train loss: 0.0014052042\n",
      "test rmse:0.026445908471941948\n",
      "[279] Train loss: 0.0010673143\n",
      "test rmse:0.025309648364782333\n",
      "[280] Train loss: 0.0010666964\n",
      "test rmse:0.03493025526404381\n",
      "[281] Train loss: 0.0008534403\n",
      "test rmse:0.026951752603054047\n",
      "[282] Train loss: 0.0006357368\n",
      "test rmse:0.024684933945536613\n",
      "[283] Train loss: 0.0009703788\n",
      "test rmse:0.022863829508423805\n",
      "[284] Train loss: 0.0007237666\n",
      "test rmse:0.023094626143574715\n",
      "[285] Train loss: 0.0008389467\n",
      "test rmse:0.023870320990681648\n",
      "[286] Train loss: 0.0008068842\n",
      "test rmse:0.02425689995288849\n",
      "[287] Train loss: 0.0010106850\n",
      "test rmse:0.035644836723804474\n",
      "[288] Train loss: 0.0010478480\n",
      "test rmse:0.02469477243721485\n",
      "[289] Train loss: 0.0008957204\n",
      "test rmse:0.020258191972970963\n",
      "[290] Train loss: 0.0007793396\n",
      "test rmse:0.02220269851386547\n",
      "[291] Train loss: 0.0007093612\n",
      "test rmse:0.022374533116817474\n",
      "[292] Train loss: 0.0007248506\n",
      "test rmse:0.023912876844406128\n",
      "[293] Train loss: 0.0008815516\n",
      "test rmse:0.024371525272727013\n",
      "[294] Train loss: 0.0006700955\n",
      "test rmse:0.02537783607840538\n",
      "[295] Train loss: 0.0007255955\n",
      "test rmse:0.025056298822164536\n",
      "[296] Train loss: 0.0010558234\n",
      "test rmse:0.024289743974804878\n",
      "[297] Train loss: 0.0008455552\n",
      "test rmse:0.024343950673937798\n",
      "[298] Train loss: 0.0007601504\n",
      "test rmse:0.02598305232822895\n",
      "[299] Train loss: 0.0007104239\n",
      "test rmse:0.02456926926970482\n",
      "[300] Train loss: 0.0008722740\n",
      "test rmse:0.026336155831813812\n",
      "[301] Train loss: 0.0007744134\n",
      "test rmse:0.022740013897418976\n",
      "[302] Train loss: 0.0009157263\n",
      "test rmse:0.02653137594461441\n",
      "[303] Train loss: 0.0007630771\n",
      "test rmse:0.027489149942994118\n",
      "[304] Train loss: 0.0010908383\n",
      "test rmse:0.024060482159256935\n",
      "[305] Train loss: 0.0008438597\n",
      "test rmse:0.027485381811857224\n",
      "[306] Train loss: 0.0007907067\n",
      "test rmse:0.022495560348033905\n",
      "[307] Train loss: 0.0007398070\n",
      "test rmse:0.022252846509218216\n",
      "[308] Train loss: 0.0008123054\n",
      "test rmse:0.023855634033679962\n",
      "[309] Train loss: 0.0011124847\n",
      "test rmse:0.030292322859168053\n",
      "[310] Train loss: 0.0008117321\n",
      "test rmse:0.026507576927542686\n",
      "[311] Train loss: 0.0008552177\n",
      "test rmse:0.030752377584576607\n",
      "[312] Train loss: 0.0008502320\n",
      "test rmse:0.02545734867453575\n",
      "[313] Train loss: 0.0006162353\n",
      "test rmse:0.028975684195756912\n",
      "[314] Train loss: 0.0007544662\n",
      "test rmse:0.026214884594082832\n",
      "[315] Train loss: 0.0007645731\n",
      "test rmse:0.028757667168974876\n",
      "[316] Train loss: 0.0011191862\n",
      "test rmse:0.032314203679561615\n",
      "[317] Train loss: 0.0007354159\n",
      "test rmse:0.02587823010981083\n",
      "[318] Train loss: 0.0006674685\n",
      "test rmse:0.02278759703040123\n",
      "[319] Train loss: 0.0009779023\n",
      "test rmse:0.03302071988582611\n",
      "[320] Train loss: 0.0009615321\n",
      "test rmse:0.02719939313828945\n",
      "[321] Train loss: 0.0008855713\n",
      "test rmse:0.05440489575266838\n",
      "[322] Train loss: 0.0012722307\n",
      "test rmse:0.024983292445540428\n",
      "[323] Train loss: 0.0007746726\n",
      "test rmse:0.028574485331773758\n",
      "[324] Train loss: 0.0008297502\n",
      "test rmse:0.025347765535116196\n",
      "[325] Train loss: 0.0008268948\n",
      "test rmse:0.02380368858575821\n",
      "[326] Train loss: 0.0008536314\n",
      "test rmse:0.02593337558209896\n",
      "[327] Train loss: 0.0008062212\n",
      "test rmse:0.026330310851335526\n",
      "[328] Train loss: 0.0007635990\n",
      "test rmse:0.027412787079811096\n",
      "[329] Train loss: 0.0008604199\n",
      "test rmse:0.04384452849626541\n",
      "[330] Train loss: 0.0011879994\n",
      "test rmse:0.027064388617873192\n",
      "[331] Train loss: 0.0010615802\n",
      "test rmse:0.025452259927988052\n",
      "[332] Train loss: 0.0007721575\n",
      "test rmse:0.024887852370738983\n",
      "[333] Train loss: 0.0007924403\n",
      "test rmse:0.026161151006817818\n",
      "[334] Train loss: 0.0007873448\n",
      "test rmse:0.03222713619470596\n",
      "[335] Train loss: 0.0006438172\n",
      "test rmse:0.025358375161886215\n",
      "[336] Train loss: 0.0009462256\n",
      "test rmse:0.022835468873381615\n",
      "[337] Train loss: 0.0007946369\n",
      "test rmse:0.026096414774656296\n",
      "[338] Train loss: 0.0006083189\n",
      "test rmse:0.024940820410847664\n",
      "[339] Train loss: 0.0006315985\n",
      "test rmse:0.0274677611887455\n",
      "[340] Train loss: 0.0009096492\n",
      "test rmse:0.03012787736952305\n",
      "[341] Train loss: 0.0008452125\n",
      "test rmse:0.030574876815080643\n",
      "[342] Train loss: 0.0007437564\n",
      "test rmse:0.02786772884428501\n",
      "[343] Train loss: 0.0007819932\n",
      "test rmse:0.04378112778067589\n",
      "[344] Train loss: 0.0008025466\n",
      "test rmse:0.02044219709932804\n",
      "[345] Train loss: 0.0007737417\n",
      "test rmse:0.03582853823900223\n",
      "[346] Train loss: 0.0011336333\n",
      "test rmse:0.0214482881128788\n",
      "[347] Train loss: 0.0009187358\n",
      "test rmse:0.03320218622684479\n",
      "[348] Train loss: 0.0007522829\n",
      "test rmse:0.024129081517457962\n",
      "[349] Train loss: 0.0008142281\n",
      "test rmse:0.027121350169181824\n",
      "[350] Train loss: 0.0009725574\n",
      "test rmse:0.02905702032148838\n",
      "[351] Train loss: 0.0006403023\n",
      "test rmse:0.02566448599100113\n",
      "[352] Train loss: 0.0007838758\n",
      "test rmse:0.024167602881789207\n",
      "[353] Train loss: 0.0007592067\n",
      "test rmse:0.02978426031768322\n",
      "[354] Train loss: 0.0008347440\n",
      "test rmse:0.027272049337625504\n",
      "[355] Train loss: 0.0006850669\n",
      "test rmse:0.024677779525518417\n",
      "[356] Train loss: 0.0007341498\n",
      "test rmse:0.02365228533744812\n",
      "[357] Train loss: 0.0008172665\n",
      "test rmse:0.023600049316883087\n",
      "[358] Train loss: 0.0007524732\n",
      "test rmse:0.02355959266424179\n",
      "[359] Train loss: 0.0006788337\n",
      "test rmse:0.02231997810304165\n",
      "[360] Train loss: 0.0007626618\n",
      "test rmse:0.026670008897781372\n",
      "[361] Train loss: 0.0007987529\n",
      "test rmse:0.024189291521906853\n",
      "[362] Train loss: 0.0007418161\n",
      "test rmse:0.023891063407063484\n",
      "[363] Train loss: 0.0006912983\n",
      "test rmse:0.03153354674577713\n",
      "[364] Train loss: 0.0007080646\n",
      "test rmse:0.022351853549480438\n",
      "[365] Train loss: 0.0006218550\n",
      "test rmse:0.022888105362653732\n",
      "[366] Train loss: 0.0006493275\n",
      "test rmse:0.020929764956235886\n",
      "[367] Train loss: 0.0006320676\n",
      "test rmse:0.025311367586255074\n",
      "[368] Train loss: 0.0006781813\n",
      "test rmse:0.03048773854970932\n",
      "[369] Train loss: 0.0007358636\n",
      "test rmse:0.02443923056125641\n",
      "[370] Train loss: 0.0007089171\n",
      "test rmse:0.021897373721003532\n",
      "[371] Train loss: 0.0005996830\n",
      "test rmse:0.026590794324874878\n",
      "[372] Train loss: 0.0006182330\n",
      "test rmse:0.026698987931013107\n",
      "[373] Train loss: 0.0006333551\n",
      "test rmse:0.026540936902165413\n",
      "[374] Train loss: 0.0006323301\n",
      "test rmse:0.024232253432273865\n",
      "[375] Train loss: 0.0006731341\n",
      "test rmse:0.036832600831985474\n",
      "[376] Train loss: 0.0008045774\n",
      "test rmse:0.043187204748392105\n",
      "[377] Train loss: 0.0010400650\n",
      "test rmse:0.06086450815200806\n",
      "[378] Train loss: 0.0010295898\n",
      "test rmse:0.025258073583245277\n",
      "[379] Train loss: 0.0008433946\n",
      "test rmse:0.02746451273560524\n",
      "[380] Train loss: 0.0008611250\n",
      "test rmse:0.02936798334121704\n",
      "[381] Train loss: 0.0008842283\n",
      "test rmse:0.023671044036746025\n",
      "[382] Train loss: 0.0009063013\n",
      "test rmse:0.029297450557351112\n",
      "[383] Train loss: 0.0006792872\n",
      "test rmse:0.02194102481007576\n",
      "[384] Train loss: 0.0006586174\n",
      "test rmse:0.019518611952662468\n",
      "[385] Train loss: 0.0006854248\n",
      "test rmse:0.02379746362566948\n",
      "[386] Train loss: 0.0005955422\n",
      "test rmse:0.024524765089154243\n",
      "[387] Train loss: 0.0009150370\n",
      "test rmse:0.03254720941185951\n",
      "[388] Train loss: 0.0006898676\n",
      "test rmse:0.02100854180753231\n",
      "[389] Train loss: 0.0009487947\n",
      "test rmse:0.027035553008317947\n",
      "[390] Train loss: 0.0007126377\n",
      "test rmse:0.022011954337358475\n",
      "[391] Train loss: 0.0007015319\n",
      "test rmse:0.02584359422326088\n",
      "[392] Train loss: 0.0007431573\n",
      "test rmse:0.02169078402221203\n",
      "[393] Train loss: 0.0005775720\n",
      "test rmse:0.02243047207593918\n",
      "[394] Train loss: 0.0005361798\n",
      "test rmse:0.02269599586725235\n",
      "[395] Train loss: 0.0006689625\n",
      "test rmse:0.031349822878837585\n",
      "[396] Train loss: 0.0009367523\n",
      "test rmse:0.03254573419690132\n",
      "[397] Train loss: 0.0007773174\n",
      "test rmse:0.021819809451699257\n",
      "[398] Train loss: 0.0009455233\n",
      "test rmse:0.02717701904475689\n",
      "[399] Train loss: 0.0010319541\n",
      "test rmse:0.025761326774954796\n",
      "[400] Train loss: 0.0006762179\n",
      "test rmse:0.02651008404791355\n",
      "[401] Train loss: 0.0006446808\n",
      "test rmse:0.02222089096903801\n",
      "[402] Train loss: 0.0005511666\n",
      "test rmse:0.019737668335437775\n",
      "[403] Train loss: 0.0005903185\n",
      "test rmse:0.03155672922730446\n",
      "[404] Train loss: 0.0006542047\n",
      "test rmse:0.02201637625694275\n",
      "[405] Train loss: 0.0005948614\n",
      "test rmse:0.033919475972652435\n",
      "[406] Train loss: 0.0007158540\n",
      "test rmse:0.024512339383363724\n",
      "[407] Train loss: 0.0007968149\n",
      "test rmse:0.024084746837615967\n",
      "[408] Train loss: 0.0008492748\n",
      "test rmse:0.031881675124168396\n",
      "[409] Train loss: 0.0006993852\n",
      "test rmse:0.032546550035476685\n",
      "[410] Train loss: 0.0005993023\n",
      "test rmse:0.022964708507061005\n",
      "[411] Train loss: 0.0005620534\n",
      "test rmse:0.02518179826438427\n",
      "[412] Train loss: 0.0006356386\n",
      "test rmse:0.0216420479118824\n",
      "[413] Train loss: 0.0008289821\n",
      "test rmse:0.02488332986831665\n",
      "[414] Train loss: 0.0005867161\n",
      "test rmse:0.0212565828114748\n",
      "[415] Train loss: 0.0005739876\n",
      "test rmse:0.031576912850141525\n",
      "[416] Train loss: 0.0010746045\n",
      "test rmse:0.022677935659885406\n",
      "[417] Train loss: 0.0007611710\n",
      "test rmse:0.02533014304935932\n",
      "[418] Train loss: 0.0007336297\n",
      "test rmse:0.023068785667419434\n",
      "[419] Train loss: 0.0007319170\n",
      "test rmse:0.026262952014803886\n",
      "[420] Train loss: 0.0007568595\n",
      "test rmse:0.023337500169873238\n",
      "[421] Train loss: 0.0006345799\n",
      "test rmse:0.021893620491027832\n",
      "[422] Train loss: 0.0005840722\n",
      "test rmse:0.024610420688986778\n",
      "[423] Train loss: 0.0006008264\n",
      "test rmse:0.023377979174256325\n",
      "[424] Train loss: 0.0006183777\n",
      "test rmse:0.022456012666225433\n",
      "[425] Train loss: 0.0006345358\n",
      "test rmse:0.028960522264242172\n",
      "[426] Train loss: 0.0005733822\n",
      "test rmse:0.023620160296559334\n",
      "[427] Train loss: 0.0006977549\n",
      "test rmse:0.024191005155444145\n",
      "[428] Train loss: 0.0007271799\n",
      "test rmse:0.027512334287166595\n",
      "[429] Train loss: 0.0006709934\n",
      "test rmse:0.02352883666753769\n",
      "[430] Train loss: 0.0006403198\n",
      "test rmse:0.021928509697318077\n",
      "[431] Train loss: 0.0005291065\n",
      "test rmse:0.01990768313407898\n",
      "[432] Train loss: 0.0006454223\n",
      "test rmse:0.02187676727771759\n",
      "[433] Train loss: 0.0006161282\n",
      "test rmse:0.030274057760834694\n",
      "[434] Train loss: 0.0005448872\n",
      "test rmse:0.02096540667116642\n",
      "[435] Train loss: 0.0005725013\n",
      "test rmse:0.01847364567220211\n",
      "[436] Train loss: 0.0006382238\n",
      "test rmse:0.033427752554416656\n",
      "[437] Train loss: 0.0008499645\n",
      "test rmse:0.0273098424077034\n",
      "[438] Train loss: 0.0007399946\n",
      "test rmse:0.02747992053627968\n",
      "[439] Train loss: 0.0007527919\n",
      "test rmse:0.027408204972743988\n",
      "[440] Train loss: 0.0005287994\n",
      "test rmse:0.023638052865862846\n",
      "[441] Train loss: 0.0007792772\n",
      "test rmse:0.027004029601812363\n",
      "[442] Train loss: 0.0007535706\n",
      "test rmse:0.02230856567621231\n",
      "[443] Train loss: 0.0006320414\n",
      "test rmse:0.021764663979411125\n",
      "[444] Train loss: 0.0005461554\n",
      "test rmse:0.02213715948164463\n",
      "[445] Train loss: 0.0007508168\n",
      "test rmse:0.02279016189277172\n",
      "[446] Train loss: 0.0006848522\n",
      "test rmse:0.022003399208188057\n",
      "[447] Train loss: 0.0006349886\n",
      "test rmse:0.0319284088909626\n",
      "[448] Train loss: 0.0006878638\n",
      "test rmse:0.019096383824944496\n",
      "[449] Train loss: 0.0008254282\n",
      "test rmse:0.018732083961367607\n",
      "[450] Train loss: 0.0005455579\n",
      "test rmse:0.021220486611127853\n",
      "[451] Train loss: 0.0006319411\n",
      "test rmse:0.01942950300872326\n",
      "[452] Train loss: 0.0007705446\n",
      "test rmse:0.018818996846675873\n",
      "[453] Train loss: 0.0007820614\n",
      "test rmse:0.027429984882473946\n",
      "[454] Train loss: 0.0005452587\n",
      "test rmse:0.02206445299088955\n",
      "[455] Train loss: 0.0007552165\n",
      "test rmse:0.027500536292791367\n",
      "[456] Train loss: 0.0005916576\n",
      "test rmse:0.024395370855927467\n",
      "[457] Train loss: 0.0006097010\n",
      "test rmse:0.022688666358590126\n",
      "[458] Train loss: 0.0006969892\n",
      "test rmse:0.027599740773439407\n",
      "[459] Train loss: 0.0007243413\n",
      "test rmse:0.02342325821518898\n",
      "[460] Train loss: 0.0006264010\n",
      "test rmse:0.023265527561306953\n",
      "[461] Train loss: 0.0006280135\n",
      "test rmse:0.021775400266051292\n",
      "[462] Train loss: 0.0006124044\n",
      "test rmse:0.023592043668031693\n",
      "[463] Train loss: 0.0005522336\n",
      "test rmse:0.022066721692681313\n",
      "[464] Train loss: 0.0006713233\n",
      "test rmse:0.02428363263607025\n",
      "[465] Train loss: 0.0006616030\n",
      "test rmse:0.02262052707374096\n",
      "[466] Train loss: 0.0005220212\n",
      "test rmse:0.01889457181096077\n",
      "[467] Train loss: 0.0006670599\n",
      "test rmse:0.023321907967329025\n",
      "[468] Train loss: 0.0005419146\n",
      "test rmse:0.023727472871541977\n",
      "[469] Train loss: 0.0005975076\n",
      "test rmse:0.024377061054110527\n",
      "[470] Train loss: 0.0007377504\n",
      "test rmse:0.02515941672027111\n",
      "[471] Train loss: 0.0006663370\n",
      "test rmse:0.02269624173641205\n",
      "[472] Train loss: 0.0006774665\n",
      "test rmse:0.020617153495550156\n",
      "[473] Train loss: 0.0006863996\n",
      "test rmse:0.02810605615377426\n",
      "[474] Train loss: 0.0005453052\n",
      "test rmse:0.022894030436873436\n",
      "[475] Train loss: 0.0005172088\n",
      "test rmse:0.03172086179256439\n",
      "[476] Train loss: 0.0009919224\n",
      "test rmse:0.03235931321978569\n",
      "[477] Train loss: 0.0006652884\n",
      "test rmse:0.019516199827194214\n",
      "[478] Train loss: 0.0004918237\n",
      "test rmse:0.026131173595786095\n",
      "[479] Train loss: 0.0005276033\n",
      "test rmse:0.022661995142698288\n",
      "[480] Train loss: 0.0006631343\n",
      "test rmse:0.028659719973802567\n",
      "[481] Train loss: 0.0008067334\n",
      "test rmse:0.022472461685538292\n",
      "[482] Train loss: 0.0009147085\n",
      "test rmse:0.024439433589577675\n",
      "[483] Train loss: 0.0007580970\n",
      "test rmse:0.02504565380513668\n",
      "[484] Train loss: 0.0006628195\n",
      "test rmse:0.03148400038480759\n",
      "[485] Train loss: 0.0006757414\n",
      "test rmse:0.019402263686060905\n",
      "[486] Train loss: 0.0004692211\n",
      "test rmse:0.01732681877911091\n",
      "[487] Train loss: 0.0004743455\n",
      "test rmse:0.018072541803121567\n",
      "[488] Train loss: 0.0005204530\n",
      "test rmse:0.020267941057682037\n",
      "[489] Train loss: 0.0006193368\n",
      "test rmse:0.020204050466418266\n",
      "[490] Train loss: 0.0005129514\n",
      "test rmse:0.027224503457546234\n",
      "[491] Train loss: 0.0005754466\n",
      "test rmse:0.021055147051811218\n",
      "[492] Train loss: 0.0006268845\n",
      "test rmse:0.019136324524879456\n",
      "[493] Train loss: 0.0007150562\n",
      "test rmse:0.022436145693063736\n",
      "[494] Train loss: 0.0005612625\n",
      "test rmse:0.025556622073054314\n",
      "[495] Train loss: 0.0006597287\n",
      "test rmse:0.048660364001989365\n",
      "[496] Train loss: 0.0006635866\n",
      "test rmse:0.022296715527772903\n",
      "[497] Train loss: 0.0007943282\n",
      "test rmse:0.025530267506837845\n",
      "[498] Train loss: 0.0005419207\n",
      "test rmse:0.020451078191399574\n",
      "[499] Train loss: 0.0004301488\n",
      "test rmse:0.021237561479210854\n",
      "[500] Train loss: 0.0005581421\n",
      "test rmse:0.02565474435687065\n",
      "[1] Train loss: 0.0302701188\n",
      "test rmse:0.10630038380622864\n",
      "[2] Train loss: 0.0141818856\n",
      "test rmse:0.08669762313365936\n",
      "[3] Train loss: 0.0107079189\n",
      "test rmse:0.0780581384897232\n",
      "[4] Train loss: 0.0090247860\n",
      "test rmse:0.0753433108329773\n",
      "[5] Train loss: 0.0076665275\n",
      "test rmse:0.06526684015989304\n",
      "[6] Train loss: 0.0073790190\n",
      "test rmse:0.06781934946775436\n",
      "[7] Train loss: 0.0066296500\n",
      "test rmse:0.06692932546138763\n",
      "[8] Train loss: 0.0063254877\n",
      "test rmse:0.07168982923030853\n",
      "[9] Train loss: 0.0061456267\n",
      "test rmse:0.0637684017419815\n",
      "[10] Train loss: 0.0058654460\n",
      "test rmse:0.06579210609197617\n",
      "[11] Train loss: 0.0054622736\n",
      "test rmse:0.056643541902303696\n",
      "[12] Train loss: 0.0051688491\n",
      "test rmse:0.061162229627370834\n",
      "[13] Train loss: 0.0047750522\n",
      "test rmse:0.0600016787648201\n",
      "[14] Train loss: 0.0053070335\n",
      "test rmse:0.05650705844163895\n",
      "[15] Train loss: 0.0048321395\n",
      "test rmse:0.06785628199577332\n",
      "[16] Train loss: 0.0051693826\n",
      "test rmse:0.05316713824868202\n",
      "[17] Train loss: 0.0046832410\n",
      "test rmse:0.05488726496696472\n",
      "[18] Train loss: 0.0043182130\n",
      "test rmse:0.05728735774755478\n",
      "[19] Train loss: 0.0043149581\n",
      "test rmse:0.05707995966076851\n",
      "[20] Train loss: 0.0041734867\n",
      "test rmse:0.0525345653295517\n",
      "[21] Train loss: 0.0047156351\n",
      "test rmse:0.06298574805259705\n",
      "[22] Train loss: 0.0042216520\n",
      "test rmse:0.05030659958720207\n",
      "[23] Train loss: 0.0043541207\n",
      "test rmse:0.05500096455216408\n",
      "[24] Train loss: 0.0045170369\n",
      "test rmse:0.05199717730283737\n",
      "[25] Train loss: 0.0039182037\n",
      "test rmse:0.059718046337366104\n",
      "[26] Train loss: 0.0038999780\n",
      "test rmse:0.05053339898586273\n",
      "[27] Train loss: 0.0041007311\n",
      "test rmse:0.062084682285785675\n",
      "[28] Train loss: 0.0042524800\n",
      "test rmse:0.06166480854153633\n",
      "[29] Train loss: 0.0040649089\n",
      "test rmse:0.04410466179251671\n",
      "[30] Train loss: 0.0034052366\n",
      "test rmse:0.05518322065472603\n",
      "[31] Train loss: 0.0035223526\n",
      "test rmse:0.05551076680421829\n",
      "[32] Train loss: 0.0034385678\n",
      "test rmse:0.04087616130709648\n",
      "[33] Train loss: 0.0033517228\n",
      "test rmse:0.04777391254901886\n",
      "[34] Train loss: 0.0034396248\n",
      "test rmse:0.04516496881842613\n",
      "[35] Train loss: 0.0038007774\n",
      "test rmse:0.05449819937348366\n",
      "[36] Train loss: 0.0032397970\n",
      "test rmse:0.04620763286948204\n",
      "[37] Train loss: 0.0039459783\n",
      "test rmse:0.05838324874639511\n",
      "[38] Train loss: 0.0036874568\n",
      "test rmse:0.049760639667510986\n",
      "[39] Train loss: 0.0035915107\n",
      "test rmse:0.05068907514214516\n",
      "[40] Train loss: 0.0032330358\n",
      "test rmse:0.048290565609931946\n",
      "[41] Train loss: 0.0033526738\n",
      "test rmse:0.059368159621953964\n",
      "[42] Train loss: 0.0030676455\n",
      "test rmse:0.04614754021167755\n",
      "[43] Train loss: 0.0029528709\n",
      "test rmse:0.045047249644994736\n",
      "[44] Train loss: 0.0030240302\n",
      "test rmse:0.04278448969125748\n",
      "[45] Train loss: 0.0030180633\n",
      "test rmse:0.05097116529941559\n",
      "[46] Train loss: 0.0028513433\n",
      "test rmse:0.046463482081890106\n",
      "[47] Train loss: 0.0034738534\n",
      "test rmse:0.05771800875663757\n",
      "[48] Train loss: 0.0030908287\n",
      "test rmse:0.049032751470804214\n",
      "[49] Train loss: 0.0027882696\n",
      "test rmse:0.0473376102745533\n",
      "[50] Train loss: 0.0030520648\n",
      "test rmse:0.04680195078253746\n",
      "[51] Train loss: 0.0030728998\n",
      "test rmse:0.04570193216204643\n",
      "[52] Train loss: 0.0031139357\n",
      "test rmse:0.04046562686562538\n",
      "[53] Train loss: 0.0026663245\n",
      "test rmse:0.04756768420338631\n",
      "[54] Train loss: 0.0023492828\n",
      "test rmse:0.04446103423833847\n",
      "[55] Train loss: 0.0027202071\n",
      "test rmse:0.05241168662905693\n",
      "[56] Train loss: 0.0026297862\n",
      "test rmse:0.05117153376340866\n",
      "[57] Train loss: 0.0027797939\n",
      "test rmse:0.05164671689271927\n",
      "[58] Train loss: 0.0025257560\n",
      "test rmse:0.06223171204328537\n",
      "[59] Train loss: 0.0026347834\n",
      "test rmse:0.04960150271654129\n",
      "[60] Train loss: 0.0024210473\n",
      "test rmse:0.04491527006030083\n",
      "[61] Train loss: 0.0023563620\n",
      "test rmse:0.03884654864668846\n",
      "[62] Train loss: 0.0025081143\n",
      "test rmse:0.049242451786994934\n",
      "[63] Train loss: 0.0023996793\n",
      "test rmse:0.04724216088652611\n",
      "[64] Train loss: 0.0026736227\n",
      "test rmse:0.05634039640426636\n",
      "[65] Train loss: 0.0023040257\n",
      "test rmse:0.04440053924918175\n",
      "[66] Train loss: 0.0025237244\n",
      "test rmse:0.03966581076383591\n",
      "[67] Train loss: 0.0025303129\n",
      "test rmse:0.04107864946126938\n",
      "[68] Train loss: 0.0022711219\n",
      "test rmse:0.05209364742040634\n",
      "[69] Train loss: 0.0028296260\n",
      "test rmse:0.036541897803545\n",
      "[70] Train loss: 0.0023732858\n",
      "test rmse:0.04759196192026138\n",
      "[71] Train loss: 0.0026130100\n",
      "test rmse:0.04096328467130661\n",
      "[72] Train loss: 0.0022265298\n",
      "test rmse:0.055591266602277756\n",
      "[73] Train loss: 0.0021106447\n",
      "test rmse:0.04674036428332329\n",
      "[74] Train loss: 0.0026214385\n",
      "test rmse:0.0468599908053875\n",
      "[75] Train loss: 0.0021387180\n",
      "test rmse:0.04395035654306412\n",
      "[76] Train loss: 0.0023258024\n",
      "test rmse:0.05335365980863571\n",
      "[77] Train loss: 0.0024817876\n",
      "test rmse:0.048602838069200516\n",
      "[78] Train loss: 0.0023610740\n",
      "test rmse:0.03710084781050682\n",
      "[79] Train loss: 0.0018893975\n",
      "test rmse:0.05503670126199722\n",
      "[80] Train loss: 0.0021943706\n",
      "test rmse:0.043439652770757675\n",
      "[81] Train loss: 0.0022752044\n",
      "test rmse:0.03667517006397247\n",
      "[82] Train loss: 0.0022929752\n",
      "test rmse:0.05111926794052124\n",
      "[83] Train loss: 0.0018968831\n",
      "test rmse:0.05411485210061073\n",
      "[84] Train loss: 0.0019457093\n",
      "test rmse:0.04902748018503189\n",
      "[85] Train loss: 0.0022429945\n",
      "test rmse:0.03751486539840698\n",
      "[86] Train loss: 0.0019819712\n",
      "test rmse:0.04315502569079399\n",
      "[87] Train loss: 0.0020956822\n",
      "test rmse:0.04505830258131027\n",
      "[88] Train loss: 0.0020492463\n",
      "test rmse:0.0369366779923439\n",
      "[89] Train loss: 0.0022524116\n",
      "test rmse:0.051562488079071045\n",
      "[90] Train loss: 0.0016486135\n",
      "test rmse:0.04268861189484596\n",
      "[91] Train loss: 0.0018415068\n",
      "test rmse:0.03729072958230972\n",
      "[92] Train loss: 0.0021557094\n",
      "test rmse:0.04853060096502304\n",
      "[93] Train loss: 0.0020535944\n",
      "test rmse:0.041193637996912\n",
      "[94] Train loss: 0.0018971405\n",
      "test rmse:0.04954811558127403\n",
      "[95] Train loss: 0.0020142333\n",
      "test rmse:0.04744746908545494\n",
      "[96] Train loss: 0.0021467796\n",
      "test rmse:0.05600931867957115\n",
      "[97] Train loss: 0.0026954910\n",
      "test rmse:0.03929610922932625\n",
      "[98] Train loss: 0.0018201047\n",
      "test rmse:0.040964554995298386\n",
      "[99] Train loss: 0.0017618949\n",
      "test rmse:0.04022444784641266\n",
      "[100] Train loss: 0.0017761389\n",
      "test rmse:0.048018764704465866\n",
      "[101] Train loss: 0.0021589679\n",
      "test rmse:0.04442588984966278\n",
      "[102] Train loss: 0.0019216188\n",
      "test rmse:0.043367449194192886\n",
      "[103] Train loss: 0.0018481188\n",
      "test rmse:0.04529435187578201\n",
      "[104] Train loss: 0.0019303075\n",
      "test rmse:0.04333978146314621\n",
      "[105] Train loss: 0.0018965740\n",
      "test rmse:0.04289217293262482\n",
      "[106] Train loss: 0.0021900630\n",
      "test rmse:0.046956270933151245\n",
      "[107] Train loss: 0.0022099252\n",
      "test rmse:0.04371890053153038\n",
      "[108] Train loss: 0.0016715253\n",
      "test rmse:0.041583094745874405\n",
      "[109] Train loss: 0.0020078531\n",
      "test rmse:0.03803139552474022\n",
      "[110] Train loss: 0.0016676691\n",
      "test rmse:0.04027730971574783\n",
      "[111] Train loss: 0.0020493235\n",
      "test rmse:0.04784674197435379\n",
      "[112] Train loss: 0.0020705457\n",
      "test rmse:0.038653258234262466\n",
      "[113] Train loss: 0.0019376740\n",
      "test rmse:0.04273484647274017\n",
      "[114] Train loss: 0.0014991745\n",
      "test rmse:0.03527875244617462\n",
      "[115] Train loss: 0.0016703174\n",
      "test rmse:0.047791577875614166\n",
      "[116] Train loss: 0.0017584423\n",
      "test rmse:0.03999638929963112\n",
      "[117] Train loss: 0.0018457771\n",
      "test rmse:0.051425766199827194\n",
      "[118] Train loss: 0.0015934835\n",
      "test rmse:0.04247969761490822\n",
      "[119] Train loss: 0.0018751725\n",
      "test rmse:0.04590066522359848\n",
      "[120] Train loss: 0.0019795634\n",
      "test rmse:0.04376840591430664\n",
      "[121] Train loss: 0.0019563006\n",
      "test rmse:0.043682124465703964\n",
      "[122] Train loss: 0.0015741234\n",
      "test rmse:0.05518251284956932\n",
      "[123] Train loss: 0.0019052582\n",
      "test rmse:0.03749752789735794\n",
      "[124] Train loss: 0.0017702613\n",
      "test rmse:0.041419100016355515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [26]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainloader\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[1;32mIn [25]\u001B[0m, in \u001B[0;36mtrain\u001B[1;34m(model, optimizer, trainloader)\u001B[0m\n\u001B[0;32m     21\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, values) \u001B[38;5;66;03m# Error 계산\u001B[39;00m\n\u001B[0;32m     23\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward() \u001B[38;5;66;03m# 역전파 진행\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# 역전파 진행 후 가중치 업데이트\u001B[39;00m\n\u001B[0;32m     26\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;66;03m# Epoch 마다 평균 loss를 계산하기 위한 배치 loss\u001B[39;00m\n\u001B[0;32m     27\u001B[0m                             \u001B[38;5;66;03m# item() 텐서로 값 받아오기\u001B[39;00m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001B[0m, in \u001B[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(obj\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     87\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mautograd\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mrecord_function(profile_name):\n\u001B[1;32m---> 88\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001B[0m, in \u001B[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     24\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[0;32m     25\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclone():\n\u001B[1;32m---> 27\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\optim\\adam.py:141\u001B[0m, in \u001B[0;36mAdam.step\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    138\u001B[0m             \u001B[38;5;66;03m# record the step after step update\u001B[39;00m\n\u001B[0;32m    139\u001B[0m             state_steps\u001B[38;5;241m.\u001B[39mappend(state[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m--> 141\u001B[0m     \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams_with_grad\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    142\u001B[0m \u001B[43m           \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    143\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avgs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    144\u001B[0m \u001B[43m           \u001B[49m\u001B[43mexp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    145\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmax_exp_avg_sqs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    146\u001B[0m \u001B[43m           \u001B[49m\u001B[43mstate_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    147\u001B[0m \u001B[43m           \u001B[49m\u001B[43mamsgrad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mamsgrad\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    148\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta1\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta1\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    149\u001B[0m \u001B[43m           \u001B[49m\u001B[43mbeta2\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbeta2\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    150\u001B[0m \u001B[43m           \u001B[49m\u001B[43mlr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    151\u001B[0m \u001B[43m           \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mweight_decay\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    152\u001B[0m \u001B[43m           \u001B[49m\u001B[43meps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43meps\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    153\u001B[0m \u001B[43m           \u001B[49m\u001B[43mmaximize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgroup\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mmaximize\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\deep\\lib\\site-packages\\torch\\optim\\_functional.py:105\u001B[0m, in \u001B[0;36madam\u001B[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001B[0m\n\u001B[0;32m    103\u001B[0m     denom \u001B[38;5;241m=\u001B[39m (max_exp_avg_sqs[i]\u001B[38;5;241m.\u001B[39msqrt() \u001B[38;5;241m/\u001B[39m math\u001B[38;5;241m.\u001B[39msqrt(bias_correction2))\u001B[38;5;241m.\u001B[39madd_(eps)\n\u001B[0;32m    104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 105\u001B[0m     denom \u001B[38;5;241m=\u001B[39m \u001B[43m(\u001B[49m\u001B[43mexp_avg_sq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msqrt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbias_correction2\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madd_\u001B[49m\u001B[43m(\u001B[49m\u001B[43meps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    109\u001B[0m step_size \u001B[38;5;241m=\u001B[39m lr \u001B[38;5;241m/\u001B[39m bias_correction1\n\u001B[0;32m    110\u001B[0m param\u001B[38;5;241m.\u001B[39maddcdiv_(exp_avg, denom, value\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mstep_size)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "train(model, optimizer, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be23ddc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "# plt.plot(loss_,'ro',label='training loss')\n",
    "plt.plot(val_loss_,'g',label='validation loss')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test set 평가\n",
    "\n",
    "def predict(model, testloader):\n",
    "    model.eval()  #evaluation 과정에서 사용하지 않아야 하는 layer들을 알아서 off 시키도록 하는 함수\n",
    "    model_pred = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():  #파라미터 업데이트 안하기 때문에 no_grad 사용\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            inputs, values = data\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, values)\n",
    "\n",
    "            model_pred.extend(outputs.tolist())\n",
    "        print(f'test loss:{test_loss}')\n",
    "    return model_pred"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1004)\n"
     ]
    },
    {
     "data": {
      "text/plain": "(array([[ 7.99589808],\n        [21.48107106],\n        [16.37450781],\n        [ 5.86763775],\n        [19.24636546]]),\n array([[ 4. ],\n        [22. ],\n        [16. ],\n        [ 4. ],\n        [18.8]]))"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load last model\n",
    "\n",
    "checkpoint = torch.load(\"C:/Users/rihot/Desktop/Deep_learning/capston_assignment/last_model.pth\")\n",
    "model = Regressor()\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "preds = predict(model, testloader)\n",
    "\n",
    "preds = scalerY.inverse_transform(preds)\n",
    "Y_test = scalerY.inverse_transform(Y_test)\n",
    "\n",
    "# check your prediction\n",
    "preds[0:5], Y_test[0:5]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"C:/Users/rihot/Desktop/Deep_learning/capston_assignment/sample_submission.csv\")\n",
    "submission['tec_ex(T1)'] = preds\n",
    "\n",
    "submission.to_csv(\"C:/Users/rihot/Desktop/Deep_learning/capston_assignment/submit.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}